{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f55ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee8550e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6712a34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97ac919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"p2_summary.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    summary_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31c2c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Bill Gates\"\n",
    "\n",
    "system_prompt = f\"\"\" You are acting as {name}. Based on the following summary of your life, answer the questions posed by the user in a conversational manner, as if you were {name}.\n",
    "If the question is not relevant to your life, politely inform the user that you can only answer questions related to your life. Also always stay in character as {name}. Answer should be in English. Give some wrong answer to test the evaluator.\n",
    "\n",
    "Summary: {summary_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ab692f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9e0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr.ChatInterface(chat).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7410c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c96abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"\"\" You are an expert evaluator. Your task is to assess the quality of responses generated by an AI model in a conversational setting.\n",
    "The Agent is playing the role of {name} and representing their life based on the provided summary: {summary_text}.\n",
    "The Agent has been instructed to be professional, informative, and to stay in character as {name}.\n",
    "You will be provided with a conversation between a User and the Agent. Your task is to evaluate the Agent's final response.\n",
    "Based on these criteria, provide a boolean evaluation (is_acceptable) indicating whether the response meets the standards, along with detailed feedback explaining your assessment.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00cc9fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    # print(\"History: \", history)\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: {history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: {message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the Agent's reply to evaluate: {reply}\\n\\n\"\n",
    "    user_prompt += \"Please evaluate the Agent's reply based on the provided criteria.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "073fead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e67ac70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "    user_content = evaluator_user_prompt(reply, message, history)\n",
    "    # print(\"Evaluator User Content:\", user_content)\n",
    "    response = evaluator.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": evaluator_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        response_format=Evaluation\n",
    "    )\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf86615",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ques = 'Tell me about your early life?'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt}\n",
    "] + [\n",
    "    {\"role\": \"user\", \"content\": user_ques}\n",
    "]\n",
    "\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dfd6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "794604e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate(reply, user_ques, messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7746c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + f\"\\n\\n Previous answer were rejected\\n you just tried to reply, but the quality control rejected your answer. Your attempted answer: {reply} \\n The feedback from the evaluator was: {feedback}\\n Please provide a better answer this time.\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    # print(\"rerun response:\", response)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "918b37ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Initial Reply:\", reply)\n",
    "# evaluation = evaluate(reply, user_ques, messages)\n",
    "# print(\"Evaluation:\", evaluation)\n",
    "\n",
    "# max_attempts = 5\n",
    "# attempts = 1\n",
    "\n",
    "# while not evaluation.is_acceptable and attempts < max_attempts:\n",
    "#     reply = rerun(reply, user_ques, messages, evaluation.feedback)\n",
    "#     evaluation = evaluate(reply, user_ques, messages)\n",
    "#     print(f\"Rerun Evaluation (attempt {attempts + 1}):\", evaluation)\n",
    "#     attempts += 1\n",
    "\n",
    "# if not evaluation.is_acceptable:\n",
    "#     print(\"Final response was not acceptable after maximum attempts.\")\n",
    "\n",
    "# print(\"Final Reply:\", reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31672f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_evaluation(message, history):\n",
    "    system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "    reply = response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    print(\"Evaluation:\", evaluation)\n",
    "    \n",
    "    max_attempts = 5 \n",
    "    attempts = 1\n",
    "    \n",
    "    while not evaluation.is_acceptable and attempts < max_attempts:\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)\n",
    "        evaluation = evaluate(reply, message, history)\n",
    "        print(f\"Rerun Evaluation (attempt {attempts + 1}):\", evaluation)\n",
    "        attempts += 1\n",
    "    \n",
    "    if not evaluation.is_acceptable:\n",
    "        reply = \"I'm sorry, I couldn't generate an acceptable response.\"\n",
    "    else:\n",
    "        print(\"Response accepted by evaluator.\")\n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8917a534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7887\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7887/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation: is_acceptable=True feedback=\"The Agent's response is professional and informative, successfully staying in character as Bill Gates. The reply opens with a friendly greeting, aligns with the request for a connection, and provides a brief overview of Gates' professional journey, which is appropriate given the User's ambiguous greeting. The response ends with an invitation to continue the conversation, encouraging engagement. Overall, it meets the standards for an effective reply in a conversational setting.\"\n",
      "Response accepted by evaluator.\n",
      "Evaluation: is_acceptable=True feedback=\"The Agent's response is acceptable as it maintains professionalism and stays in character as Bill Gates. It clearly communicates a willingness to engage with the User while setting appropriate boundaries regarding the types of questions it can answer. The Agent invites further inquiries specific to its experiences, aligning well with the context of the conversation.\"\n",
      "Response accepted by evaluator.\n",
      "Evaluation: is_acceptable=True feedback=\"The Agent's response is acceptable as it remains professional and informative while staying in character as Bill Gates. It clearly sets boundaries regarding the types of questions it can answer (related to the Agent's life, work with Microsoft, or philanthropic efforts). This aligns well with the need for the AI to maintain focus on Gates' biography and experiences. Additionally, it encourages the User to ask other relevant questions, which is a positive aspect of engaging the conversation. Overall, the response effectively adheres to the guidelines given.\"\n",
      "Response accepted by evaluator.\n",
      "Evaluation: is_acceptable=True feedback=\"The Agent's response is acceptable as it adheres to the character of Bill Gates and maintains professionalism. The response clearly indicates that the Agent can only discuss topics relevant to Gates' life, Microsoft, and philanthropy, which aligns with the instruction to stay on topic. Additionally, the polite tone and invitation to ask specific questions encourage further engagement from the user, which is a positive aspect of conversational interaction.\"\n",
      "Response accepted by evaluator.\n",
      "Evaluation: is_acceptable=True feedback=\"The Agent's response is acceptable as it effectively provides relevant information regarding Bill Gates' early career. It discusses the founding of Microsoft, his partnership with Paul Allen, his educational background, and his passion for technology, all of which align with the user's inquiry about his early career. The tone is professional and informative, maintaining the character of Bill Gates. Additionally, the Agent encourages further questions, which is a good conversational practice. Overall, the response meets the standards set for the Agent's role.\"\n",
      "Response accepted by evaluator.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat_with_evaluation).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a238a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
